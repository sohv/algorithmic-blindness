================================================================================
BASELINE COMPARISON: LLMs vs Simple Methods
================================================================================

METHODOLOGY:
- Random Baseline: Uniformly random ranges for each metric
- Heuristic Baseline: Simple rules (N↑→better, vars↓→better)
- LLM Baseline: Claude (best-performing LLM)
- Ground Truth: 100 algorithm runs per dataset-algorithm combo
- Metric: Calibrated Coverage % (% of predictions that bracket true mean)

================================================================================
RESULTS (on 51 dataset-algorithm experiments × 4 metrics = 204 predictions)
================================================================================

Method                  Coverage%  Mean Score  Perfect(0.8-1.0)  Terrible(0.0-0.2)
---                     ---------  ----------  ----------------  -----------------
1. Random Baseline      39.7%      0.431       51 (25.0%)        84 (41.2%)
2. LLM (Claude)         39.7%      0.444       45 (22.1%)        76 (37.3%)
3. Heuristic Baseline   32.8%      0.358       43 (21.1%)        101 (49.5%)

================================================================================
KEY FINDINGS
================================================================================

1. **Claude ≈ Random Guessing**
   - Claude coverage: 39.7%
   - Random coverage: 39.7%
   - Difference: 0.0 percentage points
   
   ⚠️ INTERPRETATION: Claude performs NO BETTER than flipping a coin!
   This is THE most damning evidence of algorithmic blindness.

2. **Claude Beats Heuristic Slightly**
   - Claude: 39.7%
   - Heuristic: 32.8%
   - Difference: 6.9 pp in Claude's favor
   
   But heuristic is arguably poorly designed (no dataset metadata).

3. **"Good" Predictions Are Rare**
   - Perfect scores (0.8-1.0):
     * Claude: 22.1%
     * Random: 25.0% (slightly better!)
   
   - Terrible scores (0.0-0.2):
     * Claude: 37.3%
     * Random: 41.2%

================================================================================
PAPER IMPACT
================================================================================

This baseline comparison turns the blindness finding from "LLMs are poor" into
"LLMs are NO BETTER THAN RANDOM" - a much stronger claim.

Before baseline comparison:
  "LLMs achieve only 16% calibrated coverage on full dataset"

After baseline comparison:
  "Claude ties RANDOM GUESSING at 39.7% coverage; random performs as well as
   the best LLM we tested. This demonstrates LLMs have NO understanding of
   causal algorithm performance."

This is publishable-quality evidence of failure.

================================================================================
